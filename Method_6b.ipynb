{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from numpy import savetxt\n",
    "import glob\n",
    "import rea\n",
    "import importlib\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import scale\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers import Input, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from platform import python_version\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "rmse = tf.keras.metrics.RootMeanSquaredError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading original model and activation function:')\n",
    "leakyrelu=lambda x: tf.keras.activations.relu(x, alpha=0.2)\n",
    "Original=load_model(\"./originalModel.h5\",custom_objects={'<lambda>': leakyrelu})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading orignal dataset:')\n",
    "ds=[]\n",
    "orden=[]\n",
    "total_samples = 0\n",
    "for filename in glob.glob(\"./dataSet/NNTrainingTesting-3p-3c-v7_*.csv\"):\n",
    "    temp=filename[43:-1].split(\"_\")\n",
    "    DP=temp[:1]\n",
    "    DP=''.join(DP)\n",
    "    DP=int(DP)\n",
    "    orden.append(DP)\n",
    "    if DP >= 100:\n",
    "        params = np.array(filename[49+2:-4].split(\"_\"), dtype=np.float32)\n",
    "    elif DP >= 10:\n",
    "        params = np.array(filename[49+1:-4].split(\"_\"), dtype=np.float32)\n",
    "    else:\n",
    "        params = np.array(filename[49:-4].split(\"_\"), dtype=np.float32)  \n",
    "    data = genfromtxt(filename, delimiter='\\n')\n",
    "    row = np.concatenate((data, params), axis=0)\n",
    "    ds.append(row)\n",
    "    total_samples+=1\n",
    "    \n",
    "print(\"Total samples: \", total_samples)\n",
    "\n",
    "ds = np.array(ds, dtype=np.float32)\n",
    "orden=np.array(orden, dtype=int)\n",
    "\n",
    "print(ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare dataset\n",
    "\n",
    "features=np.array(ds[:,:600])\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(features)\n",
    "\n",
    "trainData = features[:400,:]\n",
    "valData = features[400:,:]\n",
    "\n",
    "trainData=scaler.transform(trainData)\n",
    "valData=scaler.transform(valData)\n",
    "\n",
    "valData11=np.array(valData[:,0:598:3])\n",
    "valData12=np.array(valData[:,1:599:3])\n",
    "valData13=np.array(valData[:,2:600:3])\n",
    "\n",
    "valDataTensor=tf.convert_to_tensor(valData)\n",
    "valData11Tensor=tf.convert_to_tensor(valData11)\n",
    "valData12Tensor=tf.convert_to_tensor(valData12)\n",
    "valData13Tensor=tf.convert_to_tensor(valData13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Call Gradient Tape to perform Automatic Differentiation\n",
    "\n",
    "with tf.GradientTape() as grad:\n",
    "    grad.watch([valData11Tensor,valData12Tensor,valData13Tensor])\n",
    "    out=Original([valData11Tensor,valData12Tensor,valData13Tensor])\n",
    "\n",
    "gradients=grad.gradient(out,[valData11Tensor,valData12Tensor,valData13Tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate from independent layers\n",
    "part1=np.array(gradients[0],dtype=np.float32)\n",
    "part2=np.array(gradients[1],dtype=np.float32)\n",
    "part3=np.array(gradients[2],dtype=np.float32)\n",
    "newGradients=np.concatenate((part1,part2,part3),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute metric with gradients\n",
    "\n",
    "absgrad=np.abs(newGradients)\n",
    "\n",
    "importancia=np.zeros((600,1))\n",
    "\n",
    "for i in range(600):\n",
    "    importancia[i,0]=np.mean(absgrad[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(importancia)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select inputs with mean higher than a given threslhold\n",
    "vips=[]\n",
    "for i in range(600):\n",
    "    if importancia[i,0]>0.0165:\n",
    "        vips.append(i)\n",
    "vips=np.array(vips,dtype=int)\n",
    "vips=np.sort(vips)\n",
    "print(len(vips))\n",
    "print(vips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Locate the input to represent it\n",
    "\n",
    "componente=[]\n",
    "fila=[]\n",
    "punto=[]\n",
    "\n",
    "for i in range(len(vips)):\n",
    "    if vips[i]<200:\n",
    "        real=vips[i]*3\n",
    "    elif (vips[i]>=200)&(vips[i]<400):\n",
    "        real=(vips[i]-200)*3+1\n",
    "    else:\n",
    "        real=(vips[i]-400)*3+2\n",
    "\n",
    "    resto=np.remainder(real,3)\n",
    "    componente.append(resto+1)\n",
    "    for j in range(10):\n",
    "        if (real<(600-60*j))&(real>=(600-60*(j+1))):\n",
    "            fila.append(j)\n",
    "    for z in range(20):\n",
    "        if (real>=(60*(9-fila[i])+3*z))&(real<(60*(9-fila[i])+3*(z+1))):\n",
    "            punto.append(z)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print inputs' locations\n",
    "\n",
    "deltax=0.105\n",
    "deltaz=0.5555558\n",
    "\n",
    "posicion=np.zeros((len(vips),2))\n",
    "for i in range(len(vips)):\n",
    "    posicion[i,0]=-punto[i]*deltax\n",
    "    posicion[i,1]=-2.5+fila[i]*deltaz\n",
    "    \n",
    "    \n",
    "line1=np.array([[-5,7],[0,0]])\n",
    "line2=np.array([[0,0],[0.5,-4]])\n",
    "plt.scatter(posicion[:,1],posicion[:,0],color='r')\n",
    "plt.plot(line1[0],line1[1],'k--')\n",
    "plt.plot(line2[0],line2[1],'k--')\n",
    "plt.ylim(0.5,-4)\n",
    "plt.xlim(7,-5)\n",
    "plt.xlabel('z [m]',fontsize=15)\n",
    "plt.ylabel('r [m]',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete the sampling point with the remaining velocity components of each selected input\n",
    "\n",
    "datos=[]\n",
    "completos=np.ones(len(vips)*3)*-1\n",
    "for i in range(len(vips)):\n",
    "    if vips[i]<200:\n",
    "        real=vips[i]*3\n",
    "    elif (vips[i]>=200)&(vips[i]<400):\n",
    "        real=(vips[i]-200)*3+1\n",
    "    else:\n",
    "        real=(vips[i]-400)*3+2\n",
    "        \n",
    "    resto=np.remainder(real,3)\n",
    "    if (real==completos).any():\n",
    "        print('Sampling point repeated')\n",
    "    else:\n",
    "        if resto==0:\n",
    "            col=[ds[:,real],ds[:,real+1],ds[:,real+2]]\n",
    "            col=np.array(col,dtype=float)\n",
    "            col2=col.transpose()\n",
    "            completos[3*i]=real\n",
    "            completos[3*i+1]=real+1\n",
    "            completos[3*i+2]=real+2\n",
    "        elif resto==1:\n",
    "            col=[ds[:,real-1],ds[:,real],ds[:,real+1]]\n",
    "            col=np.array(col,dtype=float)\n",
    "            col2=col.transpose()\n",
    "            completos[3*i]=real-1\n",
    "            completos[3*i+1]=real\n",
    "            completos[3*i+2]=real+1\n",
    "        elif resto==2:\n",
    "            col=[ds[:,real-2],ds[:,real-1],ds[:,real]]\n",
    "            col=np.array(col,dtype=float)\n",
    "            col2=col.transpose()\n",
    "            completos[3*i]=real-2\n",
    "            completos[3*i+1]=real-1\n",
    "            completos[3*i+2]=real\n",
    "        if i==0:\n",
    "            datos=col2\n",
    "        else:\n",
    "            datos=np.append(datos,col2,axis=1)\n",
    "\n",
    "            datos=np.array(datos,dtype=float)\n",
    "            \n",
    "print(datos.shape)\n",
    "inputs=datos.shape[1]\n",
    "print(inputs/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce dataset leaving only the selected sampling points\n",
    "\n",
    "labels=np.array(ds[:,600:])/5000\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(datos)\n",
    "\n",
    "train_final_input = datos[:400,:]\n",
    "test_final_input = datos[400:,:]\n",
    "train_final_label=labels[:400,:]\n",
    "test_final_label=labels[400:,:]\n",
    "\n",
    "train_final_input=scaler.transform(train_final_input)\n",
    "test_final_input=scaler.transform(test_final_input)\n",
    "\n",
    "trainfi_Vx=np.array(train_final_input[:,0:inputs-2:3])\n",
    "trainfi_Vy=np.array(train_final_input[:,1:inputs-1:3])\n",
    "trainfi_Vz=np.array(train_final_input[:,2:inputs:3])\n",
    "\n",
    "testfi_Vx=np.array(test_final_input[:,0:inputs-2:3])\n",
    "testfi_Vy=np.array(test_final_input[:,1:inputs-1:3])\n",
    "testfi_Vz=np.array(test_final_input[:,2:inputs:3])\n",
    "\n",
    "trainfi_Vx_pd=pd.DataFrame(trainfi_Vx)\n",
    "trainfi_Vy_pd=pd.DataFrame(trainfi_Vy)\n",
    "trainfi_Vz_pd=pd.DataFrame(trainfi_Vz)\n",
    "testfi_Vx_pd=pd.DataFrame(testfi_Vx)\n",
    "testfi_Vy_pd=pd.DataFrame(testfi_Vy)\n",
    "testfi_Vz_pd=pd.DataFrame(testfi_Vz)\n",
    "\n",
    "\n",
    "trainfl_pd=pd.DataFrame(train_final_label)\n",
    "testfl_pd=pd.DataFrame(test_final_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create MLP with initial layer adapted to new number of inputs\n",
    "\n",
    "I1=Input(shape=(trainfi_Vx.shape[1],))\n",
    "I2=Input(shape=(trainfi_Vy.shape[1],))\n",
    "I3=Input(shape=(trainfi_Vz.shape[1],))\n",
    "\n",
    "h1_p1=Dense(trainfi_Vx.shape[1],activation=leakyrelu)(I1)\n",
    "h1_p2=Dense(trainfi_Vy.shape[1],activation=leakyrelu)(I2)\n",
    "h1_p3=Dense(trainfi_Vz.shape[1],activation=leakyrelu)(I3)\n",
    "h1=Concatenate()([h1_p1,h1_p2,h1_p3])\n",
    "h2=Dense(500,activation=leakyrelu)(h1)\n",
    "hdp1=keras.layers.Dropout(0.5)(h2)\n",
    "h3=Dense(420,activation=leakyrelu)(hdp1)\n",
    "hdp2=keras.layers.Dropout(0.35)(h3)\n",
    "h4=Dense(360,activation=leakyrelu)(hdp2)\n",
    "hdp3=keras.layers.Dropout(0.25)(h4)\n",
    "h5=Dense(300,activation=leakyrelu)(hdp3)\n",
    "hdp4=keras.layers.Dropout(0.15)(h5)\n",
    "h6=Dense(150,activation=leakyrelu)(hdp4)\n",
    "h7=Dense(75,activation=leakyrelu)(h6)\n",
    "h8=Dense(32,activation=leakyrelu)(h7)\n",
    "Out=Dense(3)(h8)\n",
    "\n",
    "model=keras.models.Model(inputs=[I1,I2,I3],outputs=Out)\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), metrics=[rmse, 'mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train NN\n",
    "\n",
    "history = model.fit(\n",
    "    [trainfi_Vx_pd,trainfi_Vy_pd,trainfi_Vz_pd], trainfl_pd,\n",
    "    epochs=1500,\n",
    "    verbose=1,\n",
    "    validation_split = 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history,maximo):\n",
    "  plt.figure(figsize=(16,8))\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.ylim([0, maximo])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Print training\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "plot_loss(history,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model\n",
    "results= model.evaluate([testfi_Vx_pd,testfi_Vy_pd,testfi_Vz_pd], testfl_pd, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate predictions\n",
    "predicciones=model.predict([testfi_Vx_pd,testfi_Vy_pd,testfi_Vz_pd])\n",
    "for i in range(3):\n",
    "    plt.scatter(predicciones[:,i]*5000,labels[400:,i]*5000)\n",
    "    plt.title('component S %i' %i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model and predictions\n",
    "model.save(\"./NN_M6b.h5\")\n",
    "scaledPred=predicciones*5000\n",
    "np.savetxt('./Preds_M6b.csv', scaledPred, delimiter=';', fmt='%0.6f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
